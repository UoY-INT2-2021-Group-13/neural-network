{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n",
    "import time\n",
    "\n",
    "#Debug parameters:\n",
    "PRINT_EPOCH_PROGRESS_MESSAGES = False\n",
    "  #whether to print \"Processing: 10000/50000\" messages while processing epochs\n",
    "PRINT_EPOCH_PROCESSING_TIME = False\n",
    "  #whether to print \"Processed in: 6.17 seconds\" messages after processing an epoch\n",
    "EPOCH_PRINT_STRIDE = 5\n",
    "  #number of epochs to skip printing (eg 10 will only print every 10th epoch)\n",
    "  #last epoch is always printed\n",
    "\n",
    "#Hyper-parameters:\n",
    "EPOCHS = 100\n",
    "  #number of iterations - a simple network will be around ~5 seconds per epoch, a bigger one can be ~20\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "TRANSFORM = Compose(\n",
    "    [ToTensor(),\n",
    "     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: ' + device)\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root='datasets',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=TRANSFORM\n",
    ")\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root='datasets',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=TRANSFORM\n",
    ")\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.network_stack = nn.Sequential(\n",
    "            nn.Linear(3*32*32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        output = self.network_stack(x)\n",
    "        return output\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, do_print=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch_num, (inputs, labels) in enumerate(dataloader):\n",
    "        #move tensors to correct device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Compute prediction and loss\n",
    "        prediction = model(inputs)\n",
    "        loss = loss_fn(prediction, labels)\n",
    "        # Backpropagate and optimize model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print something every 100 batches to let us know it's not dead\n",
    "        if PRINT_EPOCH_PROGRESS_MESSAGES and do_print and batch_num % 100 == 0:\n",
    "            current = batch_num * len(inputs)\n",
    "            print(\"Processed: \" + str(current) + \"/\" + str(size))\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad(): #disable gradients when not training (makes it faster)\n",
    "        for inputs, labels in dataloader:\n",
    "            #move tensors to correct device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #calculate test error\n",
    "            prediction = model(inputs)\n",
    "            total_loss += loss_fn(prediction, labels).item()\n",
    "            total_correct += (prediction.argmax(1) == labels).type(torch.float).sum().item()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    average_loss = round(total_loss / size, 5)\n",
    "    percentage_correct = 100 * round(total_correct / size, 4)\n",
    "    print(\"Accuracy: \" + str(percentage_correct) + \"%\")\n",
    "    print(\"Average Loss: \" + str(average_loss))\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "previous_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    #only print if this matches an epoch print stride, or is the last epoch\n",
    "    do_print = epoch % EPOCH_PRINT_STRIDE == 0 or epoch == EPOCHS-1\n",
    "    if do_print:\n",
    "      print(\"-----------------------------\")\n",
    "      print(\"Epoch \" + str(epoch+1))\n",
    "    train(train_dataloader, model, LOSS_FUNCTION, optimizer, do_print)\n",
    "    #only calculate results if printing them:\n",
    "    if do_print:\n",
    "      test(test_dataloader, model, LOSS_FUNCTION)\n",
    "      if PRINT_EPOCH_PROCESSING_TIME:\n",
    "        print(\"Processed in: \" + str(round(time.time() - previous_time,2)) + \" seconds\")\n",
    "        previous_time = time.time()\n",
    "print(\"finished :D\")"
   ]
  }
 ]
}