{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add some text up here at the top of the file\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, RandomCrop, RandomHorizontalFlip, Normalize, Compose, Lambda\n",
    "import time\n",
    "\n",
    "#Debug parameters:\n",
    "PRINT_EPOCH_PROGRESS_MESSAGES = False\n",
    "  #whether to print \"Processing: 10000/50000\" messages while processing epochs\n",
    "PRINT_EPOCH_PROCESSING_TIME = True\n",
    "  #whether to print \"Processed in: 6.17 seconds\" messages after processing an epoch\n",
    "EPOCH_PRINT_STRIDE = 1\n",
    "  #number of epochs to skip printing (eg 10 will only print every 10th epoch)\n",
    "  #last epoch is always printed\n",
    "ALWAYS_PRINT_CLASS_ACCURACY = True\n",
    "  #whether to print the accuracy per class (\"Accuracy for dog: 20%\") for every epoch\n",
    "  #last epoch is always printed with class accuracy\n",
    "PRINT_TRAINING_ACCURACY = True\n",
    "  #whether to print the accuracy on the training set\n",
    "  #useful for telling if a bad model is overfitting or just stupid\n",
    "\n",
    "#Hyper-parameters:\n",
    "EPOCHS = 50\n",
    "  #number of iterations - a simple network will be around ~5 seconds per epoch, a bigger one can be ~20\n",
    "LEARNING_RATE = 0.5e-3\n",
    "BATCH_SIZE = 4\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "TRAIN_TRANSFORM = Compose(\n",
    "    [RandomCrop(32, padding=4),\n",
    "     RandomHorizontalFlip(), #alter the images in ways that don't change the subject, to give us \"more\" images to learn from\n",
    "     ToTensor(),\n",
    "     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) #normalizing the data from [0,1] to [-1,1]\n",
    "TEST_TRANSFORM = Compose(\n",
    "    [ToTensor(),\n",
    "     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) #same normalization as training set\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: ' + device)\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root='datasets',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=TRAIN_TRANSFORM\n",
    ")\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root='datasets',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=TEST_TRANSFORM\n",
    ")\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') #used for printing results by class\n",
    "\n",
    "#peaks at around 67 to 66\n",
    "#~38 seconds per epoch\n",
    "class SimpleConvolutional(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvolutional, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 5),\n",
    "            #take 5x5 convolutions to turn the 3 input colour channels into 8 output channels\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #squish a channel to half size by taking the highest value in every 2x2 block\n",
    "            #this is done to keep the processing small - most of these values are very similar\n",
    "            nn.Conv2d(8, 32, 5),\n",
    "            #turn those 8 channels into 32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #squish again!\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(32 * 5 * 5, 200),\n",
    "            #take the 32 channels (now 5x5 in size due to squishing and edges being lost from no padding)\n",
    "            #and make a fullly connected neural net to make it into a single flat matrix\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            #do another linear step so the network can make some clever deductions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "            #finally reduce to 10 outputs - these are our output classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "          #take the [BATCH_SIZE, 32, 5, 5] tensor and resize it to be a [BATCH_SIZE, 32*5*5] tensor\n",
    "          #so we can do linear stuffs with it\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "#peaks at around 68 to 67\n",
    "#~38 seconds per epoch\n",
    "class WhatIfWeHadMoreLayers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WhatIfWeHadMoreLayers, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 3),\n",
    "            #30x30\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(6, 12, 3),\n",
    "            #28x28\n",
    "            nn.ReLU(),\n",
    "            #continue as before\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(12, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(32 * 5 * 5, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "#bigger NN that doesn't use pooling, and has lots more layers\n",
    "#surprisingly, this only takes 44 seconds per epoch, but it's not very good\n",
    "#it takes a few epochs to get started - it tends to picks a class and say \"every picture is this class\"\n",
    "#only reaches a peak of ~58 to ~57 or so\n",
    "#maybe there's so many layers that it can't identify useful mutations?\n",
    "class NoPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NoPooling, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 8),\n",
    "            nn.ReLU(),\n",
    "            #25x25            \n",
    "            nn.Conv2d(6, 12, 6),\n",
    "            nn.ReLU(),\n",
    "            #20x20\n",
    "            nn.Conv2d(12, 20, 5),\n",
    "            nn.ReLU(),\n",
    "            #16x16\n",
    "            nn.Conv2d(20, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            #14x14\n",
    "            nn.Conv2d(32, 50, 3),\n",
    "            nn.ReLU(),\n",
    "            #12x12\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(50 * 12 * 12, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        x = x.view(-1, 50 * 12 * 12)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class BiggerNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiggerNeuralNetwork, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        x = x.view(-1, 4096)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, do_print=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_correct = 0\n",
    "    for batch_num, (inputs, labels) in enumerate(dataloader):\n",
    "        #move tensors to correct device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Compute prediction and loss\n",
    "        prediction = model(inputs)\n",
    "        loss = loss_fn(prediction, labels)\n",
    "        # Backpropagate and optimize model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if PRINT_TRAINING_ACCURACY:\n",
    "          total_correct += (prediction.argmax(1) == labels).type(torch.float).sum().item()\n",
    "        #print something every 100 batches to let us know it's not dead\n",
    "        if PRINT_EPOCH_PROGRESS_MESSAGES and do_print and batch_num % 100 == 0:\n",
    "            current = batch_num * len(inputs)\n",
    "            print(\"Processed: \" + str(current) + \"/\" + str(size))\n",
    "    if PRINT_TRAINING_ACCURACY:\n",
    "      percentage_correct = round(100 * total_correct / size, 4)\n",
    "      print(\"Training Set Accuracy: \" + str(percentage_correct) + \"%\")\n",
    "\n",
    "def test(dataloader, model, loss_fn, do_classes=False):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    #init dictionary of classes\n",
    "    total_correct_class = {}\n",
    "    for classname in classes:\n",
    "        total_correct_class[classname] = 0\n",
    "\n",
    "    with torch.no_grad(): #disable gradients when not training (makes it faster)\n",
    "        for inputs, labels in dataloader:\n",
    "            #move tensors to correct device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #calculate test error\n",
    "            prediction = model(inputs)\n",
    "            total_loss += loss_fn(prediction, labels).item()\n",
    "            total_correct += (prediction.argmax(1) == labels).type(torch.float).sum().item()\n",
    "            if do_classes:\n",
    "                #isolate label predictions:\n",
    "                _, label_predictions = torch.max(prediction, 1)\n",
    "                #need to process these individually, can't be handled as a batch\n",
    "                for label, prediction in zip(labels, label_predictions):\n",
    "                    if label == prediction:\n",
    "                        total_correct_class[classes[label]] += 1\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    class_size = size // 10\n",
    "    if do_classes:\n",
    "        for classname, correct in total_correct_class.items():\n",
    "            percentage_correct = round(100 * correct / class_size, 4)\n",
    "            print(\"Accuracy for \" + classname + \": \" + str(percentage_correct) + \"%\")\n",
    "\n",
    "    average_loss = round(total_loss / size, 5)\n",
    "    percentage_correct = round(100 * total_correct / size, 4)\n",
    "    print(\"Accuracy: \" + str(percentage_correct) + \"%\")\n",
    "    print(\"Average Loss: \" + str(average_loss))\n",
    "\n",
    "model = BiggerNeuralNetwork().to(device)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "previous_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    #only print if this matches an epoch print stride, or is the last epoch\n",
    "    do_print = epoch % EPOCH_PRINT_STRIDE == 0 or epoch == EPOCHS-1\n",
    "    #likewise for printing class accuracies\n",
    "    do_classes = ALWAYS_PRINT_CLASS_ACCURACY or epoch == EPOCHS-1\n",
    "    if do_print:\n",
    "      print(\"-----------------------------\")\n",
    "      print(\"Epoch \" + str(epoch+1))\n",
    "    train(train_dataloader, model, LOSS_FUNCTION, optimizer, do_print)\n",
    "    #only calculate results if printing them:\n",
    "    if do_print:\n",
    "      test(test_dataloader, model, LOSS_FUNCTION, do_classes)\n",
    "      if PRINT_EPOCH_PROCESSING_TIME:\n",
    "        print(\"Processed in: \" + str(round(time.time() - previous_time,2)) + \" seconds\")\n",
    "        previous_time = time.time()\n",
    "      \n",
    "print(\"finished :D\")"
   ]
  }
 ]
}