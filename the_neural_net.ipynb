{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, RandomCrop, RandomHorizontalFlip, Normalize, Compose, Lambda\n",
    "import time\n",
    "\n",
    "#Debug parameters:\n",
    "PRINT_EPOCH_PROGRESS_MESSAGES = False\n",
    "  #whether to print \"Processing: 10000/50000\" messages while processing epochs\n",
    "PRINT_EPOCH_PROCESSING_TIME = True\n",
    "  #whether to print \"Processed in: 6.17 seconds\" messages after processing an epoch\n",
    "EPOCH_PRINT_STRIDE = 1\n",
    "  #number of epochs to skip printing (eg 10 will only print every 10th epoch)\n",
    "  #last epoch is always printed\n",
    "ALWAYS_PRINT_CLASS_ACCURACY = True\n",
    "  #whether to print the accuracy per class (\"Accuracy for dog: 20%\") for every epoch\n",
    "  #last epoch is always printed with class accuracy\n",
    "PRINT_TRAINING_ACCURACY = True\n",
    "  #whether to print the accuracy on the training set\n",
    "  #useful for telling if a bad model is overfitting or just stupid\n",
    "SAVE_NETWORK = True\n",
    "  #whether to save the produced network\n",
    "  #saves are performed after every epoch - each model has its own save file (the most recent file is saved)\n",
    "SAVE_PATH = 'models'\n",
    "  #networks are all saved to \"models/<time the model was started>\"\n",
    "LOAD_NETWORK = False\n",
    "LOAD_PATH = 'models/'\n",
    "  #whether to load from a saved network\n",
    "PROGRAM_START = time.time()\n",
    "\n",
    "#Hyper-parameters:\n",
    "EPOCHS = 100\n",
    "  #number of iterations - a simple linear network can be as fast as ~5 seconds per epoch, a bigger one can be over 100\n",
    "  #cpu tends to double the processing time\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 4\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "TRAIN_TRANSFORM = Compose(\n",
    "    [RandomCrop(32, padding=4),\n",
    "     RandomHorizontalFlip(), #alter the images in ways that don't change the subject, to give us \"more\" images to learn from\n",
    "     ToTensor(),\n",
    "     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) #normalizing the data from [0,1] to [-1,1]\n",
    "TEST_TRANSFORM = Compose(\n",
    "    [ToTensor(),\n",
    "     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) #same normalization as training set\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: ' + device)\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root='datasets',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=TRAIN_TRANSFORM\n",
    ")\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root='datasets',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=TEST_TRANSFORM\n",
    ")\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') #used for printing results by class\n",
    "\n",
    "#Base network with very few modifications from source.\n",
    "#(Unsurprisingly) hard to optimize, since the source has already done their best to optimize it. Dropout in conv block 2 is increased from 0.05 to 0.1.\n",
    "class BiggerNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiggerNeuralNetwork, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "            # Convolutional block 1\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Convolutional block 2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=0.1),\n",
    "\n",
    "            # Convolutional block 3\n",
    "            nn.Conv2d(128, 256, 5, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        x = x.view(-1, 4096)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "#Significantly increases processing time (over double!). Learning rate is noticably improved.\n",
    "#Not run for long enough to see peak, but tentatively appears to be higher? Exciting!\n",
    "class FiveKernelFinalBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FiveKernelFinalBlock, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "            # Convolutional block 1\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Convolutional block 2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(p=0.1),\n",
    "\n",
    "            # Convolutional block 3\n",
    "            nn.Conv2d(128, 256, 5, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        x = x.view(-1, 4096)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, do_print=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_correct = 0\n",
    "    for batch_num, (inputs, labels) in enumerate(dataloader):\n",
    "        #move tensors to correct device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Compute prediction and loss\n",
    "        prediction = model(inputs)\n",
    "        loss = loss_fn(prediction, labels)\n",
    "        # Backpropagate and optimize model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if PRINT_TRAINING_ACCURACY:\n",
    "          total_correct += (prediction.argmax(1) == labels).type(torch.float).sum().item()\n",
    "        #print something every 100 batches to let us know it's not dead\n",
    "        if PRINT_EPOCH_PROGRESS_MESSAGES and do_print and batch_num % 100 == 0:\n",
    "            current = batch_num * len(inputs)\n",
    "            print(\"Processed: \" + str(current) + \"/\" + str(size))\n",
    "    if PRINT_TRAINING_ACCURACY:\n",
    "      percentage_correct = round(100 * total_correct / size, 4)\n",
    "      print(\"Training Set Accuracy: \" + str(percentage_correct) + \"%\")\n",
    "\n",
    "def test(dataloader, model, loss_fn, do_classes=False):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    #init dictionary of classes\n",
    "    total_correct_class = {}\n",
    "    for classname in classes:\n",
    "        total_correct_class[classname] = 0\n",
    "\n",
    "    with torch.no_grad(): #disable gradients when not training (makes it faster)\n",
    "        for inputs, labels in dataloader:\n",
    "            #move tensors to correct device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #calculate test error\n",
    "            prediction = model(inputs)\n",
    "            total_loss += loss_fn(prediction, labels).item()\n",
    "            total_correct += (prediction.argmax(1) == labels).type(torch.float).sum().item()\n",
    "            if do_classes:\n",
    "                #isolate label predictions:\n",
    "                _, label_predictions = torch.max(prediction, 1)\n",
    "                #need to process these individually, can't be handled as a batch\n",
    "                for label, prediction in zip(labels, label_predictions):\n",
    "                    if label == prediction:\n",
    "                        total_correct_class[classes[label]] += 1\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    class_size = size // 10\n",
    "    if do_classes:\n",
    "        for classname, correct in total_correct_class.items():\n",
    "            percentage_correct = round(100 * correct / class_size, 4)\n",
    "            print(\"Accuracy for \" + classname + \": \" + str(percentage_correct) + \"%\")\n",
    "\n",
    "    average_loss = round(total_loss / size, 5)\n",
    "    percentage_correct = round(100 * total_correct / size, 4)\n",
    "    print(\"Accuracy: \" + str(percentage_correct) + \"%\")\n",
    "    print(\"Average Loss: \" + str(average_loss))\n",
    "\n",
    "model = FiveKernelFinalBlock().to(device)\n",
    "if LOAD_NETWORK:\n",
    "    model = net.load_state_dict(torch.load(LOAD_PATH))\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "previous_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    #only print if this matches an epoch print stride, or is the last epoch\n",
    "    do_print = epoch % EPOCH_PRINT_STRIDE == 0 or epoch == EPOCHS-1\n",
    "    #likewise for printing class accuracies\n",
    "    do_classes = ALWAYS_PRINT_CLASS_ACCURACY or epoch == EPOCHS-1\n",
    "    if do_print:\n",
    "      print(\"-----------------------------\")\n",
    "      print(\"Epoch \" + str(epoch+1))\n",
    "    train(train_dataloader, model, LOSS_FUNCTION, optimizer, do_print)\n",
    "    #only calculate results if printing them:\n",
    "    if do_print:\n",
    "      test(test_dataloader, model, LOSS_FUNCTION, do_classes)\n",
    "      if PRINT_EPOCH_PROCESSING_TIME:\n",
    "        print(\"Processed in: \" + str(round(time.time() - previous_time,2)) + \" seconds\")\n",
    "        previous_time = time.time()\n",
    "    if SAVE_NETWORK:\n",
    "      path = ''\n",
    "      if LOAD_NETWORK:\n",
    "        path = LOAD_PATH\n",
    "      else:\n",
    "        path = SAVE_PATH + '/' + str(program_start)\n",
    "      torch.save(model.state_dict(), path)\n",
    "      \n",
    "print(\"finished :D\")"
   ]
  }
 ]
}